(window.webpackJsonp=window.webpackJsonp||[]).push([[129],{712:function(t,v,_){"use strict";_.r(v);var n=_(5),p=Object(n.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("p",[t._v("AI 产品的诞生，我们就假设是一个模拟角色对话类的 AI 产品。")]),t._v(" "),v("p",[t._v("第一步就是模型选择，其实就是 LLM 类的模型选择，之前我总有一种错觉是，这种模型会很大，但是其实并不然，像很多模型其实并不大，至少比我想象的要小，7、8B 参数量基本上就能满足要求，这种参数量我们的自己的 PC 都能带动起来。")]),t._v(" "),v("p",[t._v("选模型的过程会占用很多时间，如果对模型要求并不高的话，其实找一些常见的模型，比如 LLama，MIstral 这些公司都有提供对应的小参数量的模型，也基本够用，但是如果对模型有一些特殊的要求的话，就需要去社区以及 HuggingFace 上找对应的版本，就有点类似在 Github 上找开源库一样。")]),t._v(" "),v("p",[t._v("我们评价Github 开源库的一个标准是 Start 数够不够多，以及社区讨论的够不够多，看维护情况之类的，但是评价开源模型似乎并不是这样，很多时候有的模型非常冷门，社区提到的很少，但是真的挺好用的。这点也比较出乎我的意料。")]),t._v(" "),v("p",[t._v("一般会选一些表现还可以的待测试的模型放到测试列表，交给产品经理去进行测试，测试的过程其实就是看是否模型的对话效果是否满足要求，比如是否遵从系统提示词的指令小于规定的回复字数之类的，以及是否能完整的 GET 到用户的对话意图，以及能主动推进对话等等...")]),t._v(" "),v("p",[t._v("测试模型是一个细致的活，需要反复的更换提示词进行测试，我之前一直对模型提示词工程师这个岗位存疑，觉得提示词有什么难的，但现在看来，调提示词还真是个技术活，考察的是对产品要求和模型的理解是否到位。")]),t._v(" "),v("p",[t._v("之前和产品聊一些模型的表现，有的产品会对模型进行 finetune，我一直以为是会重新训练模型之类的，但是产品同事回复说很多模型其实改完提示词之后的表现也会很好，这让我对提示词这件事儿产生了敬畏之情... 没想到提示词会有这么重要。")]),t._v(" "),v("p",[t._v("想要测试模型，通常需要给产品部署模型、同时搭建一个对话界面。如果是把模型下载到本地测试的话可能会影响测试进度，因为本地机器配置不够，所以通常都会在云端找一台带显卡的配置稍微高一点的机器，去下载和部署模型，提升模型的响应速度，同时搭建一个 Gradio 的界面去让产品进行带界面的测试，方便随时修改 System Prompt.")]),t._v(" "),v("p",[t._v("模型部署的方式其实有很多，有的云厂商有自带的部署方案，也有一些其他的部署方案，比如 Ollama,vllm 之类的，我自己感觉大差不差，能交互就行了呗，反正还没有遇到说哪个部署方案显著优于其他部署方案的。")]),t._v(" "),v("p",[t._v("测试完模型之后就是将上面的内容工程化，对接后端，AIGC 时代，模型侧就是后端的后端，因为数据是模型产生的，后端还是做存储和相关的业务处理。")]),t._v(" "),v("p",[t._v("现在的实践中模型侧使用了 Python 对话工程和模型对接，然后后端和Python 对话工程对接。我自己感觉理论上可以把和模型交互的部分也放到后端去做，好处很明显，因为对话工程有的时候也需要访问一些业务数据，两个服务需要互相暴露接口就很烦。不过目前因为后端是 JAVA 实现的，JAVA 侧有很多 AI 相关的库不如 Python全，所以暂时还是把这两部分分开实现了。")]),t._v(" "),v("p",[t._v("我理解差不多这就是 AI 产品在技术侧的全貌了，说起来似乎也没有太复杂的东西，其实我觉得里面唯一有技术含量的还真就是提示词那块儿，别的都是工程上的东西，只有有项目驱动学起来还是挺快的，但是看模型的效果好坏完全是看经验和对产品的理解。")])])}),[],!1,null,null,null);v.default=p.exports}}]);