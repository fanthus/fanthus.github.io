(window.webpackJsonp=window.webpackJsonp||[]).push([[126],{699:function(v,_,p){"use strict";p.r(_);var t=p(5),n=Object(t.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("p",[v._v("之前的文章聊过手机嵌入端模型的一些趋势 AI大潮来袭，移动开发迎来第二春？最近做的需求正好是将端模型引入到了我们的应用里。")]),v._v(" "),_("p",[v._v("大家知道模型种类有很多，有语言模型，生图模型，生视频模型，其实理论上来说有多少业务场景，就会有多少种模型的出现。我们做的是将 Text To Speech 端模型引入到移动端，来降低语音生成的过程对服务端的依赖，同时完成降本。\n不过在实践过程中还是踩了一些坑。")]),v._v(" "),_("p",[v._v("模型文件下载过程。要完成断点续传以及后台下载，我这次开发才知道原来 iOS 后台下载功能玄机还挺多的，甚至能在应用被杀死之后继续下载。我选择的 TTS 模型大概 300M，相对已经算是很小了。")]),v._v(" "),_("p",[v._v("感觉大部分的软件对模型文件的下载过程做的都比较糙，基本上都是在程序进程内完成下载。")]),v._v(" "),_("p",[v._v("TTS端模型支持的语言有限。我们选择的模型只支持英文，别的语言都不支持，这也是端模型使用的一个痛点。进行转换之前先检查文本是否支持生成语音。")]),v._v(" "),_("p",[v._v("另外一个端模型的痛点是，大量文本一次性生成语音的速度比较慢，这就需要端上做一些策略，比如对文本进行分割处理，批量去进行处理来提升 TTS 的生成速度。")]),v._v(" "),_("p",[v._v("还有一点不及预期的是端上初始化模型的时候，要占相当部分的内存，而且初始化模型耗时也很明显，能感受到启动的时候会卡顿一下，这部分还没有想到更有效的优化方案。")]),v._v(" "),_("p",[v._v("另外一点是解析模型的框架会增大包的安装体积，我们这次引入模型整体增加了 20M 的包大小，勉强能接受。但是我们用的不是 MLX 的模型解析框架，而是 ONNX，如果后续引入 MLX 支持的模型，还需要再引入 MLX 相关的框架，可能还会继续增加包大小。")]),v._v(" "),_("p",[v._v("感觉说了半天都是说端模型不太好的点，但是端模型好处也很明显，那就是不需要依赖服务端，整个生成语音的过程完全是离线进行的，也不需要在意用户的网络环境。")]),v._v(" "),_("p",[v._v("端模型嵌入手机带来另外一个好处是，简化了服务端的逻辑，提升了后端整体消息的响应速度。")]),v._v(" "),_("p",[v._v("我今天还尝试用了下 iOS 里面语音备忘录，最近更新的版本里支持了录音转文本的功能，我看也是下载了半天数据，大概率也是去下载模型了，苹果这么做大概率是为了安全。")]),v._v(" "),_("p",[v._v("不过我发现嵌入端模型这件事儿本质上并不需要对模型有特别深入的理解，对应的 API 底层其实已经都封装好了，我们直观调用就好了，其实能调试的空间挺少的，模型配置的参数也很有限。")]),v._v(" "),_("p",[v._v("聊一百遍不如做一遍，真的做一遍之后才发现，端模型想要真正大面积应用在手机上，手机性能还是要再提升一下，模型能力也对应要提升。")]),v._v(" "),_("p",[v._v("今天看 Qwen3.0 发了之后，我看有 0.6B 的模型，应该是可以跑在手机上，还没来得及试，回头试试看效果如何。")])])}),[],!1,null,null,null);_.default=n.exports}}]);